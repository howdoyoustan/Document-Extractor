{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bba2abe-212a-4e71-b681-bd34a60b14da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\ddev\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: werkzeug in c:\\users\\ddev\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\ddev\\anaconda3\\lib\\site-packages (8.3.223)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\ddev\\anaconda3\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\ddev\\anaconda3\\lib\\site-packages (1.26.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\n",
      "ERROR: No matching distribution found for cv2\n"
     ]
    }
   ],
   "source": [
    "pip install flask werkzeug ultralytics opencv-python-headless PyMuPDF cv2 ultralytics pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f151f4-32b1-4c51-9e03-1a7406439f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 # OpenCV for image manipulation\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Part 1: Training ---\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Trains a new YOLOv8 model.\n",
    "    This function is run separately *before* you start the web app.\n",
    "    It looks for your dataset in 'dataset/data.yaml'.\n",
    "    \"\"\"\n",
    "    print(\"Starting YOLOv8 model training...\")\n",
    "    \n",
    "    # Load a pre-trained YOLOv8 'small' model\n",
    "    model = YOLO('yolov8s.pt') \n",
    "    \n",
    "    # Train the model\n",
    "    # data: Path to your data.yaml file\n",
    "    # epochs: How many times to go over the data. Start with 100.\n",
    "    # imgsz: Resize images to this size for training. 640 is standard.\n",
    "    results = model.train(data='dataset/data.yaml', epochs=100, imgsz=640)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Model saved to: {results.save_dir}\")\n",
    "    print(f\"Your 'best.pt' model is ready. Update MODEL_PATH in app.py if needed.\")\n",
    "    \n",
    "    # After training, it's good practice to validate\n",
    "    model.val()\n",
    "\n",
    "\n",
    "# --- Part 2: Inference ---\n",
    "\n",
    "def run_inference(model_path, source_image_path, face_output_dir):\n",
    "    \"\"\"\n",
    "    Runs inference using your trained model on a single image.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to your 'best.pt' trained model.\n",
    "        source_image_path (str): Path to the new image to process.\n",
    "        face_output_dir (str): Directory to save the cropped face image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dict: extracted_data, str: face_image_path)\n",
    "               - extracted_data: {'name': '...', 'dob': '...', 'id_number': '...'}\n",
    "               - face_image_path: Relative path to the saved face crop (e.g., 'static/faces/face_1.png')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load your custom-trained model\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}. Did you train it?\")\n",
    "        raise e\n",
    "\n",
    "    # Run inference\n",
    "    results = model(source_image_path)\n",
    "\n",
    "    # --- Process Results ---\n",
    "    \n",
    "    # `results` is a list of Results objects. We take the first one.\n",
    "    result = results[0]\n",
    "    \n",
    "    # Original image for cropping\n",
    "    img = cv2.imread(source_image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Could not read source image: {source_image_path}\")\n",
    "\n",
    "    # Get class names from the model\n",
    "    class_names = result.names\n",
    "    \n",
    "    extracted_data = {}\n",
    "    face_image_path = None\n",
    "    \n",
    "    # We will use OCR on the cropped boxes.\n",
    "    # For a real production app, you'd use a robust OCR library\n",
    "    # like Tesseract (pytesseract) or an API (Google Vision).\n",
    "    # For this example, we'll just placeholder the text extraction.\n",
    "    \n",
    "    for box in result.boxes:\n",
    "        # Get class ID and confidence\n",
    "        class_id = int(box.cls[0])\n",
    "        class_name = class_names[class_id]\n",
    "        confidence = float(box.conf[0])\n",
    "        \n",
    "        # Get bounding box coordinates (xyxy format)\n",
    "        x1, y1, x2, y2 = [int(coord) for coord in box.xyxy[0]]\n",
    "        \n",
    "        # Crop the detected region from the original image\n",
    "        cropped_img = img[y1:y2, x1:x2]\n",
    "        \n",
    "        # --- Handle Face vs. Text Fields ---\n",
    "        \n",
    "        if class_name == 'face':\n",
    "            if confidence > 0.5: # Only save high-confidence faces\n",
    "                # Create a unique filename for the face\n",
    "                base_filename = os.path.splitext(os.path.basename(source_image_path))[0]\n",
    "                face_filename = f\"face_{base_filename}_{x1}.png\"\n",
    "                \n",
    "                # Full path to save the face\n",
    "                full_save_path = os.path.join(face_output_dir, face_filename)\n",
    "                \n",
    "                # Save the cropped face image\n",
    "                cv2.imwrite(full_save_path, cropped_img)\n",
    "                \n",
    "                # Store the *relative* path for use in HTML\n",
    "                face_image_path = os.path.join(face_output_dir, face_filename).replace('\\\\', '/')\n",
    "                print(f\"Saved face to: {face_image_path}\")\n",
    "                \n",
    "        else:\n",
    "            # --- Placeholder for OCR ---\n",
    "            # In a real app, you would pass `cropped_img` to an OCR function.\n",
    "            # text = run_ocr(cropped_img) \n",
    "            # For now, we'll use a placeholder.\n",
    "            # ---------------------------\n",
    "            text = f\"[OCR result for {class_name}]\"\n",
    "            \n",
    "            print(f\"Detected: {class_name} (Conf: {confidence:.2f})\")\n",
    "            \n",
    "            # Store the extracted text (or placeholder)\n",
    "            # This handles finding the *best* detection if there are duplicates\n",
    "            if class_name not in extracted_data or confidence > extracted_data.get(f\"{class_name}_conf\", 0):\n",
    "                extracted_data[class_name] = text\n",
    "                extracted_data[f\"{class_name}_conf\"] = confidence\n",
    "\n",
    "    \n",
    "    # Clean up the data dict to only contain text values\n",
    "    final_data = {key: val for key, val in extracted_data.items() if not key.endswith('_conf')}\n",
    "    \n",
    "    return final_data, face_image_path\n",
    "\n",
    "# --- Simple OCR Stub (Example) ---\n",
    "#\n",
    "# To make this real, install `pytesseract` and `tesseract-ocr`:\n",
    "# pip install pytesseract\n",
    "# (You also need to install the Tesseract binary: https://github.com/tesseract-ocr/tesseract)\n",
    "#\n",
    "# import pytesseract\n",
    "#\n",
    "# def run_ocr(image):\n",
    "#     \"\"\"Runs OCR on a CV2 image.\"\"\"\n",
    "#     try:\n",
    "#         # Pre-process image for better OCR (e.g., grayscale, threshold)\n",
    "#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#         _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "#         \n",
    "#         # Run Tesseract\n",
    "#         text = pytesseract.image_to_string(thresh, config='--psm 6')\n",
    "#         return text.strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"OCR Error: {e}\")\n",
    "#         return \"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
